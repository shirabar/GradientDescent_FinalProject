\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{epstopdf}
\usepackage[colorlinks=true, bookmarks=true]{hyperref}
\usepackage{bibtopic,wrapfig}
\usepackage{bbding}
\usepackage{listings}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


%\usepackage{refcheck}
\setcounter{MaxMatrixCols}{10}

\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\DeclareMathOperator*{\argmin}{argmin} % thin space, limits underneath in displays


\begin{document}
\lstset{language=Matlab,%
    %basicstyle=\color{red},
    breaklines=true,%
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    %numbers=left,%
    %numberstyle={\tiny \color{black}},% size of the numbers
    %numbersep=9pt, % this defines how far the numbers are from the text
    emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},
}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

%\center % Center everything on the page

%----------------------------------------------------------------------------------------
%   HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE ORT Braude College}\\[1.5cm] % Name of your university/college
\textsc{\LARGE Department of Mathematics}\\[0.5cm] % Major heading such as course name
\textsc{\LARGE B.sc. final year project}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Data Fitting and Classification methods }\\[0.4cm] % Title of your document
\HRule \\[1.5cm]

%----------------------------------------------------------------------------------------
%   AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Shira Bar-Dov % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Prof. Aviv Gibali  % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%   DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%   LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[scale=0.3]{Braude_Logo2.png}\\[1cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


%----------------------------------------------------------------------------------------
%   Thanks SECTION
%----------------------------------------------------------------------------------------
%\bfseries
\rule{\linewidth}{0.1mm}\\[0.4cm]
{ \huge  \emph{I would like to thank...}}\\[0.4cm] % Title of your document
\rule{\linewidth}{0.1mm}\\[0.4cm]
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%   Table of contents SECTION
%----------------------------------------------------------------------------------------
\newpage
\tableofcontents
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%   List of figures SECTION
%----------------------------------------------------------------------------------------
\newpage

\begin{abstract}
Fista\\
We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse
problems arising in signal/image processing. This class of methods, which can be viewed as an extension
of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for
solving large-scale problems even with dense matrix data. However, such methods are also known to
converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm
(FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence
which is proven to be significantly better, both theoretically and practically. Initial promising numerical
results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is
shown to be faster than ISTA by several orders of magnitude.
We describe and analyze a simple and effective stochastic sub-gradient
descent algorithm for solving the optimization problem cast by Support Vector
Machines (SVM). We prove that the number of iterations required to obtain a solution
of accuracy , where each iteration operates on a single training
example. In contrast, previous analyses of stochastic gradient descent methods for
SVMs require
 iterations. As in previously devised SVM solvers, the number
of iterations also scales linearly with 1=, where  is the regularization parameter
of SVM. For a linear kernel, the total run-time of our method is
where d is a bound on the number of non-zero features in each example. Since
the run-time does not depend directly on the size of the training set, the resulting
algorithm is especially suited for learning from large datasets. Our approach
also extends to non-linear kernels while working solely on the primal objective
function, though in this case the runtime does depend linearly on the training set
size. Our algorithm is particularly well suited for large text classification problems,
where we demonstrate an order-of-magnitude speedup over previous SVM
learning methods.\\

\end{abstract}

\newpage
\section{Introduction}
In this project we are concern with ...

Example of some mathematical formula.
\begin{equation}\label{P:CFPP}
x^{\ast }\in \cap _{i=1}^{m}.
\end{equation}

Example of a list

\begin{enumerate}
  \item First
  \item Second
\end{enumerate}

Example of including an image
\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{Braude_Logo2.png}
\caption{Some caption}
\label{fig:Hyper-plane}
\end{figure}

Here is how I cite a reference \cite{BC11}.

\begin{theorem}\label{th:cuttersAndQNE} Let $U:\mathcal{H}\rightarrow\mathcal{H}$
be an operator having a fixed point and let $\alpha\in(0,2]$. Then $U$ is a
cutter if and only if its $U_\alpha$ is $(2-\alpha)/\alpha$-strongly quasi-nonexpansive.
\end{theorem}

\begin{proof}
From the definition of $\alpha$-strongly quasi-nonexpansive, we get for all $(x,z)\in\mathcal{H}\times\operatorname{Fix}(U)$, that

\end{proof}


If we give a label to some object, such as equation, we can not have a cross reference by using \eqref{P:CFPP}.

\begin{algorithm}\label{alg:Online_Block}$\left. {}\right. $\textbf{Some scheme}

\textbf{Initialization}: Let $x^{0}\in\mathcal{H}$ be arbitrary starting point.\\

\textbf{Iterative step}: Given the current iterate $x^{k}$, define $N_k\in\mathbb{N}$ (number of iterations) compute the next iterate by
\begin{equation}
x^{k+1}=T_k(x^{k})
\end{equation}
\end{algorithm}


This is how to include references.
\section{Data Fitting}
In this section we will talk about first order methods of gradient descent algorithms. \\

ISTA is an extension of the classical gradient method. Therefore, ISTA belongs to the class of first order methods, that is, optimization methods that are based on function values and gradient evaluations.
It is well known that for large-scale problems first order methods are often the only practical option, but the sequence ${x_k}$ converges quite slowly to a solution. Without a proof, ISTA behaves like $O(1/k)$.

Consider the unconstrained minimization problem:
$$(P)\;\;\; min\{F(x)\equiv f(x) + g(x) : x \in R^n \} $$

The following assumptions are made:\\
$ *\; g: R^n \rightarrow R$ is a continuous convex function which is possibly nonsmooth.\\
$ *\; f: R^n \rightarrow R$ is a smooth convex function of the type $C^{1,1}$ i.e., continuously differentiable with Lipschitz continuous gradient $L(f)$:

$$\| \bigtriangledown f(x) - \bigtriangledown f(y) \| \leq L(f) \|x-y\| \;\;\;for \;every\; x,y \in R^n $$

where $\| \cdot \|$ denotes the standard Euclidean norm and $L(f) > 0$ is the Lipschitz constant
of $\bigtriangledown f$.\\

When $g(x) \equiv0$, $(P)$ is the general unconstrained smooth convex minimization problem $min\left\{f(x): x\in R^n \right\}$.\\

When $f(x) = \|Ax-b\|^2,\;g(x) \equiv \| x \|_1 $, $(P)$ is the $l1$ regularization problem $min\left\{f(x) + \lambda \|x\|_1 : x\in R^n \right\}$. The (smallest) Lipschitz constant of the gradien $ \bigtriangledown f$ is $L(f) =2\lambda _{max}(A^T A)$.\\

One of the simplest methods for solving $(P)$ is in the class of iterative shrinkage-thresholding algorithms (ISTA).

For any $L > 0$, consider the following quadratic approximation of $F(x) := f(x) + g(x)$ at a given point $y$:

$$Q_L (x,y) := f(y) + <x-y, \bigtriangledown f(y)> + \frac{L}{2} \|x-y\|^2 +g(x)$$

which admits a unique minimizer

$$P_L(y) := argmin \left\{Q_L(x,y) :x \in R^n\right\}$$
Simple algebra shows that (ignoring constant terms in y)

$$P_L (y) =\argmin_x \left\{g(x) + \frac{L}{2} \left\| x-\left(y-\dfrac{1}{L}\bigtriangledown f(y)\right)\right\|^2\right\}$$

than, the gradient algorithm ISTA generates a sequence $\{x_k\}$ via

$$ x_0 \in R^n,\; x_k =P_L(x_{k-1})$$

where $L>0$ plays the role of a stepsize.\\
to conclude,  the basic iteration of ISTA for solving problem $(P)$ with constant stepsize is:
$L:=L(f)$- A Lipschitz constant of $\triangledown f$.\\
Take $x_0 \in R^n$
For each $k\geq 1$ compute $x_k = P_L(x_{k-1})$\\

%The descent lemma:\\ %%%%%%%%%%%%
%%%%%
$\textbf{FISTA: A fast iterative shrinkage-thresholding algorithm.}\\1


\newpage
\begin{thebibliography}{99}

\bibitem{BC11} Bauschke, H.H., Combettes, P.L.: Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, Berlin
(2011).
 
\end{thebibliography}
\section*{Matlab Code}

\lstinputlisting{myfun.m}


\end{document}
